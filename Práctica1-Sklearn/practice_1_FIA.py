import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os


"""
EXPERIMENT PARAMETERS

The following parameters should be modified only when indicated in the practice document.
"""
SEED = 0                             # Random seed. This must be changed to your DNI/Passport number.
TEST_SPLIT_FOR_RETENTION = 0.1              # Share of data to be used to test when using the retention method
NUMBER_OF_FOLDS = 5                         # K value when applying Cross Validation.
FOLDER_TO_SAVE_IMAGES= "./output_images"    # Where will be saved each image generated by the script.
MLP_LAYERS_STRUCTURE = (50,50)              # Layer structures used by the Multilayer Perceptron.
TRAIN_MLP = True                            # Do we train an MLP? If False, a Decision Tree should be trained.
USE_CROSS_VALIDATION = False                # Do we use Cross Validation? If False, Retention method will be used to validate model.
FEATURES_TO_USE = ["mean perimeter"]        # Features from data to be used.

if not os.path.exists(FOLDER_TO_SAVE_IMAGES):
    os.makedirs(FOLDER_TO_SAVE_IMAGES)

"""
Accuracy is not always the best metric to measure how good a trained model is, so we will get several metrics.
Following the information from https://en.wikipedia.org/wiki/Precision_and_recall, obtain Recall, Precision and F1.
"""

def get_metrics(true_labels, pred_labels):

    TP = np.count_nonzero(true_labels & pred_labels)
    TN = np.count_nonzero(~(true_labels | pred_labels))
    combined = true_labels.astype(int) - pred_labels.astype(int)
    FP = np.sum(combined == -1)
    FN = np.sum(combined == 1)

    accuracy = (TP+TN)/(TP+TN+FP+FN)
    recall = TP/(TP+FN)
    precision = TP/(TP+FP)
    if (precision + recall == 0):
        f1 = 0
    else:
        f1 = 2*precision*recall/(precision+recall)

    return accuracy, recall, precision, f1

"""
Often it's useful sometimes to print the confussion matrix for a trained model.
https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay
"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

def get_confusion_matrix(test_labels, test_prediction, labels, folder_to_save = FOLDER_TO_SAVE_IMAGES, seed = SEED, fold = None):
    cm = confusion_matrix(test_labels, test_prediction, labels=labels)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    disp.plot(cmap='Blues', values_format='d')
    # ¿Están al revés los savefig?
    if fold is not None:
        plt.savefig(f"{folder_to_save}/confusion_matrix_retention_seed_{seed}.png")
    else:
        plt.savefig(f"{folder_to_save}/confusion_matrix_fold_{fold}_seed_{seed}.png")


def retention_method(model, data, labels, seed):

    """
    SPLIT THE DATA INTO TRAIN AND TEST SUBSETS

    To use the retention method you must split the data, so you only need to use the following function from sklearn.
    https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
    Only arguments you should worry about aside from data and label are test_size and random_state.
    """

    from sklearn.model_selection import train_test_split
    train_data, test_data, train_labels, test_labels = train_test_split(data, labels, shuffle=True,
                                                                        random_state=seed, test_size=TEST_SPLIT_FOR_RETENTION)

    """
    TRAIN

    To train a model you only need to use the fit method every model from scikit-learn library has.
    """

    model.fit(train_data, train_labels) # Here there is a point followed by an ellipsis (...).

    """
    TEST

    After the training has been completed, the model should have learned how to predict labels for data it has not trained with.
    First step here is to predict labels for test_data. Once a prediction has been obtained for each test data, we can obtain
    metrics to measure how well the training went. Make sure both test_prediction and test_labels are numpy arrays.
    """

    test_prediction = np.asarray(model.predict(test_data))
    test_labels = np.asarray(test_labels)

    accuracy, recall, precision, f1 = get_metrics(test_labels, test_prediction)

    print(f"Accuracy={accuracy}, recall={recall}, precision={precision} and F1={f1}.")

    get_confusion_matrix(test_labels, test_prediction, [0,1])

"""
Cross Validation
In order to evaluate a training approach, the division of data into training and test subsets can lead to erroneous conclusions due to chance. 
In order to make the conclusions more reliable, there is the Cross Validation technique. In this technique, several training and test divisions are created, 
training is performed, metrics are obtained for each of them and finally an average of the metrics obtained is made.
"""

def cross_validation(model, data, labels, K, seed):
    """
    Use KFold to implement your own cross validation for a given model instance.
    https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html
    """
    from sklearn.model_selection import KFold
    kf = KFold(n_splits=K, shuffle=True, random_state=seed)

    acc_list = []
    recall_list = []
    precision_list = []
    f1_list = []

    for i, (train_index, test_index) in enumerate(kf.split(data)):
        print(f"Fold {i}")
        # Get the train and splits data and labels.
        train_data = data[train_index]
        test_data = data[test_index]
        train_labels = labels[train_index]
        test_labels = labels[test_index]

        # Fit the model.
        model.fit(train_data, train_labels) # Here there is a point followed by an ellipsis (...).

        # Predict.
        test_prediction = model.predict(test_data)
        # Remember to turn test_prediction and test_labels into numpy arrays if they are not.
        test_prediction = np.asarray(test_prediction)
        test_labels = np.asarray(test_labels)

        accuracy, recall, precision, f1 = get_metrics(test_labels, test_prediction)

        # Include each obtained metric into the according list.
        acc_list.append(accuracy) # Here there is a point followed by an ellipsis (...).
        recall_list.append(recall) # Here there is a point followed by an ellipsis (...).
        precision_list.append(precision) # Here there is a point followed by an ellipsis (...).
        f1_list.append(f1) # Here there is a point followed by an ellipsis (...).

        print(f"Accuracy={accuracy}, recall={recall}, precision={precision} and F1={f1}.")

        get_confusion_matrix(test_labels, test_prediction, [0,1], fold=i)

    # Obtain the average for each list of metrics.
    average_accuracy = np.mean(acc_list)
    average_recall = np.mean(recall_list)
    average_precision = np.mean(precision_list)
    average_f1 = np.mean(f1_list)

    print(f"Provided model with cross validation K={K} gets accuracy={average_accuracy}, recall={average_recall}, precision={average_precision} and FNR={average_f1}.")

    return average_accuracy, average_recall, average_precision, average_f1

"""
LOAD DATA

Load breast cancer dataset included in scikit learn. This is a binary dataset, so the label feature (cancer Yes o cancer No) can only be 1 or 0.
Follow the following example to load the data from sklearn.
https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer
Remember to load it as a Pandas dataframe and with (data, labels) format.
"""

from sklearn.datasets import load_breast_cancer
data, labels = load_breast_cancer(return_X_y=True, as_frame=True)

print("List of features included in the dataset:")
print(list(data.columns))
print("The table with the data looks as follows:")
print(data)
print("Labels vector must includes a binary clasification for each of them.")
print(f"We have a total of {data.shape[0]} examples and {labels.shape[0]} labels with values {labels.unique()}.")

"""
CLEAN DATA

Now data should be cleaned (discard or fill missing data, use some technique to balance the amount of examples for each label, etc).
In this exercies, remember to filter the dataframe to use only features in FEATURES_TO_USE.
"""

data = data[FEATURES_TO_USE]

"""
CHATGPT RECOMIENDA NORMALIZAR LOS DATOS. Sin embargo como solo se trabaja incialmente con "mean perimeter" no es muy importante.

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
data = scaler.fit_transform(data)
"""

if TRAIN_MLP:
    """
    Now you will instantiate the models.
    In this practice we'll train a Multilayer Perceptron Classifier and Decision Tree
    you can follow the example from the following link to instantiate a MLP:
    https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html
    Only arguments you need to worry about when initializing the model is hidden_layer_sizes and random_state. Both of them are in Section 0.
    """
    from sklearn.neural_network import MLPClassifier

    model = MLPClassifier(hidden_layer_sizes=MLP_LAYERS_STRUCTURE, random_state=SEED)


else:
    """
    Instantiate the decision tree.
    https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html
    Remember to provide SEED as random_state. 
    """

    from sklearn.tree import DecisionTreeClassifier
    model = DecisionTreeClassifier(random_state=SEED)

if USE_CROSS_VALIDATION:
    """
    TRAIN USING CROSS VALIDATION
    """
    cross_validation(model, data, labels, NUMBER_OF_FOLDS, SEED)
else:

    """
    TRAIN USING RETENTION METHOD
    """
    retention_method(model, data, labels, SEED)



